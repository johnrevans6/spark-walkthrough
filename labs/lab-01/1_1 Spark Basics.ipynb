{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Spark Application\n",
    "The main entry point into a Spark application is what is known as the Spark Context. The Spark Context represents the connection to the Spark cluster, and is what we use to define our Spark job. It's also what Spark uses to create Resilient Distributed Datasets (RDDs). We import the SparkContext from the pyspark library and denote the context with the variable name `sc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from tests import Lab01_Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = Lab01_Tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the Spark UI. This will become an exceedingly important dashboard to inspect and lives at localhost:4040."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Acquainted with the Spark UI\n",
    "1. Visit http://localhost:4040\n",
    "2. Answer the below questions and check your answers by running the appropriate cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: What is the default scheduler Spark is using?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = 'ANSWER_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.part_a(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: What version of Spark is the VM running?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'ANSWER_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.part_b(version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Where are the event logs located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ANSWER_HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.part_c(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map, Filter, and Reduce\n",
    "RDD's implement three transformations that you'll want to be familiar with; map(), filter(), and reduce(). Suppose I wanted to take a given string, and count up the number of occurences of each letter. It's a trivial example, but it illustrates the core of how these transformations work in Spark.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map\n",
    "The map() transformation takes a given RDD and transforms it into another RDD using some custom code that we as the developer specify. First, let's create an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = 'The rain in Spain falls mainly on the plain.'\n",
    "rdd = sc.parallelize(list(message))\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create RDD's using the parrallelize() method. This tells the Spark Context to create a distributed dataset that can be operated on in parallel. Let's map the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = rdd.map(lambda x: (x, 1))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to take a look at what's inside the RDD? We need to be careful here. Since the RDD is distributed, we need to collect its contents and return them back to a single machine one element at a time. This is generally a very slow operation, and could potentially result in an Out of Memory exception. For this small RDD, it's fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there's a problem. We have some extra characters we probably don't want, namely space and period. How do we deal with these?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter\n",
    "Recall the caveat we had with collect(). If the RDD is large, then the driver program might crash. We can filter down to subset of data using the filter() transformation. Let's use the filter() transformation to create an RDD that doesn't contain any special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = tokens.filter(lambda token: token[0] is not ' ' and token[0] is not '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to compute our letter count. To do this, we'll turn to the reduceByKey() transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce\n",
    "The reduceByKey() transformation takes a pair of key value pairs, and calculates a reduction that we define. In this case, the reduction is going to reduce each character pair down to a single key containing the letter, and then a sum of occurences of that letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = filtered_tokens.reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sortBy(lambda x: x[1], ascending=False).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Only not quite. We still have a bug. Note that S, s, T, and t are all counted as separate characters. That isn't quite what we're looking for. Let's fix this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix the RDD\n",
    "1. Map the the buggy RDD to a new RDD that contains only lower case characters\n",
    "2. Reduce and sort the new RDD from highest occurence to lowest occurence\n",
    "3. Collect the results\n",
    "4. Run the Test Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_rdd = 'ANSWER_HERE' # Write a single line of the form result.map().reduceByKey().sortBy().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.part_d(fixed_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File I/O\n",
    "Up to this point we've been working with a single string. This; however, isn't a good use case for Spark since a single string easily fits in main memory. Using Spark's built in textFile() method, we can read in one or more plain text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file_rdd = sc.textFile('data/long_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command reads in the works of William Shakespeare. Like before, the result is an RDD which we can apply map(), filter(), and reduce() transforms to. Suppose we wanted to know the five most common words ever written by William Shakespeare. We could solve this problem by accomplishing the following:\n",
    "1. Create a similar map to our previous example \n",
    "2. Filter out all of the special characters \n",
    "3. Reducing the result \n",
    "4. Sort the result in descending order\n",
    "5. Use the take() method to return the top five most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['From', 'fairest', 'creatures', 'we', 'desire', 'increase,'], 1),\n",
       " (['That', 'thereby', \"beauty's\", 'rose', 'might', 'never', 'die,'], 1),\n",
       " (['But', 'as', 'the', 'riper', 'should', 'by', 'time', 'decease,'], 1),\n",
       " (['His', 'tender', 'heir', 'might', 'bear', 'his', 'memory:'], 1),\n",
       " (['But', 'thou', 'contracted', 'to', 'thine', 'own', 'bright', 'eyes,'], 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_rdd = text_file_rdd.map(lambda x: (x.split(), 1))\n",
    "map_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm that doesn't look right, now does it? Instead of creating individual tokens, we've created a list of words that occurred for that given line. We need to flatten this result somehow so that when we map it, we get what we expect. Spark provides a special transformation for this called flatMap()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['From',\n",
       " 'fairest',\n",
       " 'creatures',\n",
       " 'we',\n",
       " 'desire',\n",
       " 'increase,',\n",
       " 'That',\n",
       " 'thereby',\n",
       " \"beauty's\",\n",
       " 'rose']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_rdd = text_file_rdd.flatMap(lambda line: line.split())\n",
    "map_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. This is something we can definitely map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = map_rdd.filter(str.isalnum).map(lambda x: (x.lower(), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And our reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 27458), ('and', 25916), ('i', 19540), ('to', 18656), ('of', 17877)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_rdd = words_rdd.reduceByKey(lambda x, y: x + y)\n",
    "results_rdd.sortBy(lambda x: x[1], ascending=False).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it. The most commonly used word in William Shakespeare's extensive vocabulary is the word \"the\". That's... less than interesting. Let's take this result and introduce my favorite datastructure in Spark, the Dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes\n",
    "In Spark, a dataframe is simply a collection of RDD's that have been organized into columns. What makes this powerful is that we can now treat our data more like a table and write SQL-like queries to filter and transform the data. This isn't quite as performant as working on the raw RDD, but it is much more user friendly. Let's revisit the first couple of steps, and make a couple of minor changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_rdd = text_file_rdd.flatMap(lambda line: line.split())\n",
    "words_rdd = map_rdd.filter(str.isalnum).filter(lambda x: len(x) > 3).map(lambda x: (x.lower(), ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of reducing our key value pairs, we're going to create a Spark Session from our Spark Context, and then use this session to create our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = SparkSession(sc)\n",
    "df = session.createDataFrame(words_rdd, ['word'])\n",
    "df.createOrReplaceTempView(\"word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create a dataframe, we specify the names of our columns as list, and then we register our new dataframe as a table we can query. In this case, I've chosen to name the column \"word\", and the table \"word_count\". Use this information to write a SQL query that answers the question, what are the top five most used words in Shakespeare's vocabulary? How about Shakespeare's top five least used words? Note that we've already filtered words that might be considered less interesting (ie words of fewer than 3 characters), so there's no need to write a convoluted WHERE class to filter out uninteresting words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_used = session.sql(\"\"\"YOUR ANSWER HERE\"\"\").take()\n",
    "\n",
    "least_used = session.sql(\"\"\"YOUR ANSWER HERE\"\"\").take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.part_e(most_used, least_used)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py3)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
