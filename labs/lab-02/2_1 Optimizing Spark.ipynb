{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from tests import Lab02_Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = Lab02_Tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reaquainting yourself with the Spark UI\n",
    "In Lab 1_1, we briefly explored the Spark UI to get a feel where basic information about our Spark configuration is held. In this lab, we'll be making heavier use of the Spark UI to help debug and potentially optimize common issues around Spark jobs. Recall that in order to open the Spark UI, you visit http://localhost:4040/jobs. \n",
    "\n",
    "### The /jobs page in detail\n",
    "Here we see the event timeline which is a rolling timeseries for running jobs. Below this, we see a list of all jobs, when they were submitted, how long a given job took, and whether or not tasks succeeded:\n",
    "\n",
    "![jobs](assets/jobs.png)\n",
    "\n",
    "\n",
    "Clicking on a job takes you to a more detailed summary of what stages executed for that particular job, as well as a visualization of the DAG Spark assembled for executing these stages:\n",
    "\n",
    "![stages](assets/stages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this information to help us troubleshoot jobs that may be running non-optimally.\n",
    "\n",
    "### Observing a long running Spark job\n",
    "Let's create a long-running Spark job. Run the following cell (it'll take around 15 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_rdd = sc.parallelize(range(1, 10000000))\n",
    "squared_rdd = slow_rdd.map(lambda x: x**2)\n",
    "squared_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now click on the job in the Spark UI. Next, click on the Stage. And expand the Event Timeline. You'll see something like this: ![timeline](assets/timeline.png)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The event timeline at this level represents a step-by-step execution of the job. In this case we see that most of our time is spent by the Executor computing our result (about 12 seconds). Let's try to speed this up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving Performance\n",
    "### take()\n",
    "One issue we should immediately note is that we are calling collect() on a sufficiently large RDD. Recall that collect() attempts to return the _entire_ RDD to the driver program. If the RDD is large enough, this could take a long time, or fail outright. For this reason, use caution when applying the collect() action. A better alternative is the take() action. take() allows you to return a list of `n` elements from the RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_rdd = sc.parallelize(range(1, 10000000))\n",
    "squared_rdd = fast_rdd.map(lambda x: x**2)\n",
    "squared_rdd.take(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This job is much faster, taking milliseconds instead of seconds. Note; however, that while the result appears ordered in this case, don't count on this. The take() method retrieves elements from all worker nodes which means the order of the elements that gets returned can't be guaranteed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### takeOrdered()\n",
    "Suppose I do need a guarantee on the order though. If we assume the RDD is sorted, then takeOrdered() will return the top `n` elements. To demonstrate this, let's revisit 1_1. Write a Spark job that determines the top 10 words in Shakespeare's vocabulary. Implement this using sortBY() and collect() first, and then compare the performance using takeOrdered(). There's a simple unit test that take a list of tuples as input to make sure you have filtered the top ten correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.part_a(hash(str('RDD_HERE')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serializing files\n",
    "File IO is a major bottleneck when working with Apache Spark. The best way to deal with this is to work with serialized files opposed to human-readable files within a pipeline. In this section we'll create a parquet file, and then compare the read performance of reading a parquet file versus reading a plain text file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, complete the following steps:\n",
    "1. Create a SparkSession object\n",
    "2. Read in the long_file.txt into an RDD\n",
    "3. Apply the flatMap() transform to split into individual words \n",
    "3. Filter out non-alpha characters\n",
    "4. Convert the resulting RDD into a dataframe with a single column\n",
    "5. Run the test case to verify the RDD contains the expected number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)\n",
    "rdd = sc.textFile('../data/long_file.txt').flatMap(lambda line: line.split()).filter(str.isalpha)\n",
    "schema = Row('word')\n",
    "df = rdd.map(schema).toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.part_b(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the processing for the file. It takes a second+ to process about 64kb of data. Let's see if a serialized file gives us any performance improvement. First, we'll write the df to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('../data/outputs/out.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll read it back in and check out the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.load('../data/outputs/out.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From seconds+ to milliseconds. Note; however, that the conversion process is expensive. So there's an initial upfront cost to serialize the raw text file, and then reading in the serialized file grants us significant performance gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse\n",
    "Spark offers the capability to optimize jobs by caching intermediate results, which we can then use in later transformations. This means that instead of creating a complex graph with redundant transformations, we can perform a single transform, cache it, and then reuse it later in the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The persist() method allows us to persist RDD's and DataFrames to memory, disk, or both. The following summarizes the capabilities of each:\n",
    "* __MEMORY_ONLY__: RDD is stored as a deserialized object directly in the JVM. If the RDD is bigger than the JVM's memory, then some or all of the partitions are dumped, and recomputed when needed.\n",
    "* __MEMORY_AND_DISK__: RDD is stored as a deserialized object directly in the JVM. If the RDD is bigger than the JVM's memory, then some or all of the partitions are spilled to disk.\n",
    "* __DISK_ONLY__: RDD is stored to directly to disk\n",
    "* __MEMORY_ONLY_SER__: RDD is stored as serialized object directly in the JVM. If the RDD is bigger than the JVM's memory, then some or all of the partitions are dumped, and recomputed when needed.\n",
    "* __MEMORY_AND_DISK_SER__: RDD is stored as a serialized object directly in the JVM. If the RDD is bigger than the JVM's memory, then some or all of the partitions are spilled to disk. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To persist an RDD or DataFrame we first import the StorageLevel mmodule from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to persist, we call the persist() method and pass in the desired StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.persist(StorageLevel.MEMORY_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then to reclaim that memory, we can either call unpersist(), or let the cache invalidate. Spark's cache implements a Least Recently Used (LRU) caching strategy. This means that the cache entry with the earliest mtime is invalidated first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we intend to persist to memory only, we can call the cache() method. cache() is a built-in method that utilizes StoageLevel.MEMORY_ONLY under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can assert whether an RDD or DataFrame is cached by checking the storageLevel.useMemory property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.storageLevel.useMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[word: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.storageLevel.useMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Sharing\n",
    "Our last optimization concept is the idea of variable sharing. Consider a scenario where we need a copy of the same variable across all executors. Spark supports this requirement in two ways. Broadcast variables, and Accumulators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcast Variable Sharing\n",
    "Broadcast variables are read-only variables that are cached to the executors one time, insteadof being shipped with each Task. This means local network overhead, which means jobs running more optimaly. Keep in mind that since these variables exist in the executor's cache, it is entirely possible that a cache eviction invalidates the variable under the LRU strategy. Thus, we favor small broadcast variables, opposed to large. Let's look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_versions = {\n",
    "    'Python':'PySpark', \n",
    "    'Scala':'Spark', \n",
    "    'R':'RSpark'\n",
    "}\n",
    "\n",
    "broadcast_versions = spark.sparkContext.broadcast(spark_versions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're creating a dictionary of the different Spark versions by programming language, and then broadcasting it to the executors. We can access the values of the broadcasted versions by accessing the `value` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySpark'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcast_versions.value['Python']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators\n",
    "Accumulators are write-only variables that are cached to the executors one time. When an accumulator gets updated, it immediately returns the updated result back to the driver program instead of adding a bunch of shuffling overhead between workers. Unlike broadcast variables, the `value` property is only accessible by the driver program. This is particularly useful for creating counters for validating the success or failure of Spark jobs. Here's an example of what this might look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum=spark.sparkContext.accumulator(0)\n",
    "rdd=spark.sparkContext.parallelize(range(1, 100, 2))\n",
    "rdd.foreach(lambda x: accum.add(1))\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we create an accumulator initialized to a value 0. Next, we create an RDD and then for each element in the rdd, we increment the accumulator by 1. Finally, we output the value of the counter and see that we have called the accumulator 50 times. This is much faster than calling count() on the RDD, and we can further extend accumulators into methods for tracking bad or inconsistent records, dropped records, even duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py3)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
