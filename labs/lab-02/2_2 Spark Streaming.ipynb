{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Streaming\n",
    "Spark supports near real-time streams of data using what it calls a Discretized (DStream). DStreams are an abstraction layer that represents a continous stream of data. This means that we need some kind of event queuing system such as Kafka serving events in order to fully realize Spark's real-time capabilities. In this lab, we'll simulate an event queue by creating a directory, and having Spark listen for when this directory gets updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Before we dive too deeply into Spark Streaming, we first need to instantiate a SparkSession() and configure it to not output parquet file metadata. Recall back to when we learned how to serialize files in 2_1. If you looked at the directory that gets materialized, you would have noticed that Spark writes several metadata files. We can disable this by setting `parquet.enable.summary-metadata` to \"false\". Next, you probably also noticed a file that takes up 0 bytes simply called SUCCESS. This file is basically a marker that lets us know the files were written successfully. This is useful when launching jobs on a cluster, but for simple local runs, it's safe to disable. To do that, we'll also set `mapreduce.fileoutputcommitter.marksuccessfuljobs` to \"false\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)\n",
    "spark.conf.set(\"parquet.enable.summary-metadata\", \"false\")\n",
    "spark.conf.set(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As finally bit of setup, we'll go ahead and use our newly created SparkSession to create a DataFrame from our long file. In this case, Spark is smart enough to split the data newline, so we don't need to specify a schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text('../data/long_file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the StreamingContext\n",
    "Just like batch processing, streaming processing requires a context to interact with the cluster manager and worker nodes. Similar to a SparkSession, a StreamingContext relies on a SparkContext. Also similar to a SparkContext, you may only have one active StreamingContext per Spark job. In this case, we'll create a StreamingContext, and set its batch polling interval to 5 seconds. Setting the batck polling interval basically establishes how often the StreamingContext will check the event queue for new files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, batchDuration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating a DStream\n",
    "DStreams support several different kinds of streaming sources. Here's a nonexhaustive list:\n",
    "\n",
    "* socketTextStream - Listens for data streaming from a TCP source\n",
    "* textFileStream - Listens for files being added to a target directory\n",
    "* queueStream - Listens for RDDs being added to a queue of RDD's\n",
    "\n",
    "For this lab, we'll implement a textFileStream that listens for new files being created in a target directory called `event_queue`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stream = ssc.textFileStream('../data/event_queue/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll implement a basic word count, and then print our results using the built-in method `pprint()`. pprint() is a method provided by the StreamingContext that allows us to print out the results of any data that was processed during a given polling interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rdd = words_stream.flatMap(lambda x: x.split()).filter(str.isalpha).filter(lambda x: len(x) > 3)\n",
    "reduced_rdd = words_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "reduced_rdd.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming data\n",
    "We intialize a stream by calling `start()` on our StreamingContext. The moment this method executes, the textFileStream will begin watching the event_queue directory for new files until we call the `stop()` method. Note that calling stop() normally will also unload the SparkContext.\n",
    "\n",
    "Since Jupyter treats each cell as a blocking operation by default, this portion of the lab is implemented all in the same cell. We first call `start()` to start the stream. Next, we randomly sample .1% of the DataFrame we created earlier, and then we write the sample in parquet format to the event_queue every five seconds. Finally, we close the stream, leaving the SparkContext active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:25:25\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:25:35\n",
      "-------------------------------------------\n",
      "('LUCIUS', 2)\n",
      "('with', 5)\n",
      "('army', 2)\n",
      "('GOTHS', 2)\n",
      "('drums', 2)\n",
      "('Whose', 1)\n",
      "('fresh', 1)\n",
      "('repair', 1)\n",
      "('thou', 1)\n",
      "('labouring', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:25:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:25:45\n",
      "-------------------------------------------\n",
      "('COPYRIGHT', 3)\n",
      "('WORLD', 3)\n",
      "('what', 1)\n",
      "('doth', 1)\n",
      "('thee', 1)\n",
      "('gave', 1)\n",
      "('breaking', 1)\n",
      "('Spanish', 1)\n",
      "('Thracian', 1)\n",
      "('bids', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:25:50\n",
      "-------------------------------------------\n",
      "('SCENE', 2)\n",
      "('seemly', 1)\n",
      "('raiment', 1)\n",
      "('those', 1)\n",
      "('nine', 1)\n",
      "('which', 1)\n",
      "('rhymers', 1)\n",
      "('Into', 1)\n",
      "('rvice', 1)\n",
      "('What', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:25:55\n",
      "-------------------------------------------\n",
      "('William', 2)\n",
      "('forbid', 1)\n",
      "('thee', 1)\n",
      "('most', 1)\n",
      "('heinous', 1)\n",
      "('thou', 1)\n",
      "('love', 1)\n",
      "('have', 1)\n",
      "('from', 1)\n",
      "('your', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:26:00\n",
      "-------------------------------------------\n",
      "('Still', 1)\n",
      "('losing', 1)\n",
      "('when', 1)\n",
      "('self', 1)\n",
      "('Doth', 1)\n",
      "('rose', 1)\n",
      "('youth', 1)\n",
      "('rightly', 1)\n",
      "('unless', 1)\n",
      "('some', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:26:05\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:26:10\n",
      "-------------------------------------------\n",
      "('William', 3)\n",
      "('those', 1)\n",
      "('same', 1)\n",
      "('tongues', 1)\n",
      "('that', 1)\n",
      "('give', 1)\n",
      "('thee', 1)\n",
      "('thine', 1)\n",
      "('nothings', 1)\n",
      "('should', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:26:15\n",
      "-------------------------------------------\n",
      "('COPYRIGHT', 3)\n",
      "('WORLD', 3)\n",
      "('living', 1)\n",
      "('record', 1)\n",
      "('your', 1)\n",
      "('fair', 1)\n",
      "('flower', 1)\n",
      "('rank', 1)\n",
      "('smell', 1)\n",
      "('shall', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:26:20\n",
      "-------------------------------------------\n",
      "('those', 1)\n",
      "('tears', 1)\n",
      "('pearl', 1)\n",
      "('which', 1)\n",
      "('love', 1)\n",
      "('against', 1)\n",
      "('self', 1)\n",
      "('tongue', 1)\n",
      "('doth', 1)\n",
      "('publish', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:26:25\n",
      "-------------------------------------------\n",
      "('ELECTRONIC', 3)\n",
      "('MACHINE', 2)\n",
      "('READABLE', 2)\n",
      "('COPIES', 2)\n",
      "('Calls', 1)\n",
      "('back', 1)\n",
      "('lovely', 1)\n",
      "('April', 1)\n",
      "('nature', 1)\n",
      "('gentle', 1)\n",
      "...\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2021-02-04 23:26:30\n",
      "-------------------------------------------\n",
      "('Enter', 3)\n",
      "('Each', 1)\n",
      "('trifle', 1)\n",
      "('under', 1)\n",
      "('truest', 1)\n",
      "('bars', 1)\n",
      "('rocks', 1)\n",
      "('impregnable', 1)\n",
      "('well', 1)\n",
      "('that', 1)\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc.start()\n",
    "\n",
    "for i in range(5):\n",
    "    sample = df.sample(0.001)\n",
    "    sample.write.format('parquet').mode('overwrite').save('../data/event_queue/')\n",
    "    time.sleep(5)\n",
    "\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark (Py3)",
   "language": "",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
